---
title: "Learning Stream 005 - Project"
author: "Michael Ebner"
date: "01/03/2018"
output: md_document
---

# Objective

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

# Analysis

## Analysis steps

. Load the data and required packages
. Clean data
. Correlation Matrix
. Partitioning the data
. Train models
. Model comparison
. Submission
. Appendix

## Load the data and required packages

In this step we get all the packages needed for this task.
Furthermore we load the data in case it's not yeat stored in the working directory.

```{r Load the data and required packages}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(ggplot2)
library(corrplot)
setwd("/Users/mebner/Documents/for_me/R_coursera/GitHub/practical-machine-learning")


if (!file.exists("train.csv")){
  fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileURL, "train.csv", method="curl")
}  
if (!file.exists("test.csv")) { 
  fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileURL, "test.csv", method="curl")
}
```


## Clean data

First we unify corrupted values like NAs, empty cells or DIV/0!.

Second we are removing unnecassary columns that
. are mostly empty (< 60%)
. contain non numerica data which can't be used for predicting



```{r Clean data}
# Replace all missing fields with NA 
train <- read.csv("train.csv",na.strings=c("NA","DIV/0!",""))
test <- read.csv("test.csv",na.strings=c("NA","DIV/0!",""))

# Remove first 7 columns which can't be used for predicting
train   <-train[,-c(1:7)]
test <-test[,-c(1:7)]

#Remove columns with more than 60% NAs
nzv_col <- nearZeroVar(train)
train <- train[, -nzv_col]

corrupted_col <- sapply(train, function(x) {sum(!(is.na(x) | x == ""))})
null_col <- names(corrupted_col[corrupted_col < 0.6 * length(train$classe)])
train <- train[, !names(train) %in% null_col]
```

# Correlations

First we want to look at the correlations in order to gain some information that might be helpful when it comes to pick one ore multiple algorythms for our model.

```{r Correlation}
corrplot(cor(train[-53]),tl.col="black",title= "Correlation plot of the variables used", order = "hclust",tl.cex = 0.5,tl.pos="l")
```

We see some strong correlations which helps us when picking the models.

# Running the Models

## Picking the algorythms

So are using multiple numeric predictors to predict a non numeric variable that leaves us with a bunch of potential algorythms to work with to name some of them:

. Kernel SVM
. Random Forest
. Neural Network
. Gradient Boosting Tree
. Decision Tree
. Logistic Regression
. Naive Bayes
. Linear SVM

We'll pick two out of them one that is fast and anotherone that might give us a higher level of Accuracy

. Speed -> Decision Tree
. Accuracy -> Random Forest


## Partition the data

```{r Running the Models: Subsetting the data}
subs <- createDataPartition(y=train$classe, p=0.75, list=FALSE)
subtrain <- train[subs, ] 
subtest <- train[-subs, ]
```


## 1st Approach: Decision Tree

It is a type of supervised learning algorithm that is mostly used for classification problems. It works for both categorical and continuous dependent variables. The it makes it easy to explain the steps of the prediction, however it's not known to be very accurate.

```{r Running the Models: 1st Approach: Decision Tree}
#Decision Tree
mod1 <- rpart(classe ~ ., data=subtrain, method="class")
pred1 <- predict(mod1, subtest, type = "class")
confusionMatrix(pred1, subtest$classe)
```


## 2nd Approach: Random Forrest

RF can handle correlated variables pretty well, that's why comes in handy here. It develops losts of decsion trees based on random selection of data and random selection of variables.

This sounds as it requires some calculation effort and that's exactly the trade off: Accuracy VS Speed.

```{r Running the Models: 2nd Approach: Random Forrest}

#Random Forest
mod2 <- randomForest(classe ~. , data=subtrain, method="class")
pred2 <- predict(mod2, subtest)
confusionMatrix(pred2, subtest$classe)
```


# Comparison

It's save to say that the random forrest method is performing better. It's much more accurate: 99.59% compared to 71.88%.
The out of sample error is < 0.005 (1 - accuracy for predictions made against the cross validation subset = 1 - 0.9959 = < 0.005).

## Submission

Finally we can use the selected model (RF) to predict the outcomes of the original test data set.

```{r Submission}
# predict the outcome of the original testing data set
predict_final <- predict(mod2, test, type="class")
predict_final
```

# Appendix

## Decision Tree visualisation

```{r Appendix: Decision Tree visualisation}
rpart.plot(mod1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
```


